{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8a69134e-38f1-4729-8c93-0faa5d461c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Bidirectional\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.callbacks import EarlyStopping \n",
    "from keras.losses import CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e0f58-f4eb-4742-8fd6-b0cc8694b4c4",
   "metadata": {},
   "source": [
    "1. Text Cleaning\n",
    "2. Put <BOS> tag and <EOS> tag for decoder input\n",
    "3. Make Vocabulary (VOCAB_SIZE)\n",
    "4. Tokenize Bag of words to Bag of IDs\n",
    "5. Padding (MAX_LEN)\n",
    "6. Word Embedding (EMBEDDING_DIM)\n",
    "7. Reshape the Data depends on neural network shape\n",
    "8. Split Data for training and validation, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9786c9c8-876e-42cb-b786-89a18ce1cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    pass\n",
    "\n",
    "def text2seq(train, label, VOCAB_SIZE):\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, split=' ', filters='')\n",
    "    tokenizer.fit_on_texts(train + label)\n",
    "    encode_seq = tokenizer.texts_to_sequences(train)\n",
    "    decode_seq = tokenizer.texts_to_sequences(label)\n",
    "    dictionary = tokenizer.word_index\n",
    "    \n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for k, v in dictionary.items():\n",
    "        if v < VOCAB_SIZE:\n",
    "            word2idx[k] = v\n",
    "            idx2word[v] = k\n",
    "        if v >= VOCAB_SIZE-1:\n",
    "            continue\n",
    "          \n",
    "    return word2idx, idx2word, encode_seq, decode_seq\n",
    "\n",
    "def padding(seq, MAX_LEN):\n",
    "    encode_seq = pad_sequences(seq, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')\n",
    "    return encode_seq\n",
    "\n",
    "def embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, MAX_LEN, embedding_matrix):\n",
    "    embedding_layer = Embedding(input_dim = VOCAB_SIZE, \n",
    "                                output_dim = EMBEDDING_DIM,\n",
    "                                input_length = MAX_LEN,\n",
    "                                weights = [embedding_matrix], trainable = False)\n",
    "    return embedding_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f934c19c-d482-4dee-a1be-4b004aade336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_sen(x):\n",
    "    return re.findall(r\"sin|cos|tan|\\d|\\w|\\(|\\)|\\+|-|\\*+\", x.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e2f0209-e74e-4bcb-b7d0-7528261ba2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.txt') as file:\n",
    "    raw = file.readlines()\n",
    "train = []\n",
    "label = []\n",
    "for row in raw:\n",
    "    row = row.strip('\\n')\n",
    "    _train, _label = row.split('=')\n",
    "    _train = reg_sen(_train)\n",
    "    _label = reg_sen(_label)\n",
    "    _label = ['#'] + _label + ['$']\n",
    "    train.append(' '.join(_train))\n",
    "    label.append(' '.join(_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a11346c-4e0f-4a0a-89d5-6c772ec9147b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['( 7 - 3 * z ) * ( - 5 * z - 9 )', '- 9 * s ** 2', '( 2 - 2 * n ) * ( n - 1 )', 'x ** 2', '( 4 - x ) * ( x - 2 3 )']\n",
      "['# 1 5 * z ** 2 - 8 * z - 6 3 $', '# - 9 * s ** 2 $', '# - 2 * n ** 2 + 4 * n - 2 $', '# x ** 2 $', '# - x ** 2 + 2 7 * x - 9 2 $']\n"
     ]
    }
   ],
   "source": [
    "print(train[:5])\n",
    "print(label[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f20095b2-9476-4acf-9ac7-2cea79256d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "train_unique = len(set(' '.join(train).split(' ')))\n",
    "label_unique = len(set(' '.join(label).split(' ')))\n",
    "\n",
    "MAX_LEN = 29\n",
    "VOCAB_SIZE = max(train_unique, label_unique)+1\n",
    "print(VOCAB_SIZE)\n",
    "EMBEDDING_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e437f126-6179-4327-9721-f5d6fd34156d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of word2idx: 34\n",
      "len of idx2word 34\n"
     ]
    }
   ],
   "source": [
    "word2idx, idx2word, encode_seq, decode_seq = text2seq(train, label, VOCAB_SIZE)\n",
    "print('len of word2idx:', len(word2idx))\n",
    "print('len of idx2word', len(idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db61545b-7ec6-4b28-9f6b-a8e015e9605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx[' '] = 0\n",
    "idx2word[0] = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "670865ad-80fd-4ef3-8f57-c32d6a591e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./weights/word2idx.json', 'w') as file:\n",
    "    json.dump(word2idx, file)\n",
    "with open('./weights/idx2word.json', 'w') as file:\n",
    "    json.dump(idx2word, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "200a88fb-9b94-4832-bd71-25c8a9277a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', '7', '-', '3', '*', 'z', ')', '*', '(', '-', '5', '*', 'z', '-', '9', ')']\n",
      "['(', '7', '-', '3', '*', 'z', ')', '*', '(', '-', '5', '*', 'z', '-', '9', ')']\n",
      "['#', '1', '5', '*', 'z', '**', '2', '-', '8', '*', 'z', '-', '6', '3', '$']\n",
      "['#', '1', '5', '*', 'z', '**', '2', '-', '8', '*', 'z', '-', '6', '3', '$']\n",
      "['-', '9', '*', 's', '**', '2']\n",
      "['-', '9', '*', 's', '**', '2']\n",
      "['#', '-', '9', '*', 's', '**', '2', '$']\n",
      "['#', '-', '9', '*', 's', '**', '2', '$']\n",
      "['(', '2', '-', '2', '*', 'n', ')', '*', '(', 'n', '-', '1', ')']\n",
      "['(', '2', '-', '2', '*', 'n', ')', '*', '(', 'n', '-', '1', ')']\n",
      "['#', '-', '2', '*', 'n', '**', '2', '+', '4', '*', 'n', '-', '2', '$']\n",
      "['#', '-', '2', '*', 'n', '**', '2', '+', '4', '*', 'n', '-', '2', '$']\n",
      "['x', '**', '2']\n",
      "['x', '**', '2']\n",
      "['#', 'x', '**', '2', '$']\n",
      "['#', 'x', '**', '2', '$']\n",
      "['(', '4', '-', 'x', ')', '*', '(', 'x', '-', '2', '3', ')']\n",
      "['(', '4', '-', 'x', ')', '*', '(', 'x', '-', '2', '3', ')']\n",
      "['#', '-', 'x', '**', '2', '+', '2', '7', '*', 'x', '-', '9', '2', '$']\n",
      "['#', '-', 'x', '**', '2', '+', '2', '7', '*', 'x', '-', '9', '2', '$']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    encode_tmp = encode_seq[i]\n",
    "    decode_tmp = decode_seq[i]\n",
    "    print([idx2word[k] for k in encode_tmp])\n",
    "    print(train[i].split(' '))\n",
    "    print([idx2word[k] for k in decode_tmp])\n",
    "    print(label[i].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8f6db25-cfd0-4d0b-be2e-f423dd9c4565",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_input = padding(encode_seq, MAX_LEN)\n",
    "decode_input = padding(decode_seq, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "029e6f7f-e552-4b2e-84c6-c6253daa0ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_array = np.full((decode_input_raw.shape[0], 1), word2idx['#'])\n",
    "# decode_input = np.append(start_array, decode_input_raw, 1)\n",
    "\n",
    "# end_array = np.full((decode_input.shape[0], 1), word2idx['$'])\n",
    "# decode_target = np.append(decode_input_raw, end_array , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fa5cbe5-c7c0-4c29-a9c1-66668a39053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros((len(train), MAX_LEN, VOCAB_SIZE), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(train), MAX_LEN, VOCAB_SIZE), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(label), MAX_LEN, VOCAB_SIZE), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88ac634c-56d9-4cdc-b6b5-9a127b2609c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse the input and output texts\n",
    "for i, (_encode, _decode) in enumerate(zip(encode_input, decode_input)):\n",
    "    for t, char in enumerate(_encode):\n",
    "        encoder_input_data[i, t, char] = 1.\n",
    "    for t, char in enumerate(_decode):\n",
    "        decoder_input_data[i, t, char] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t-1, char] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ae0f412-d58c-40af-bfe1-7ca3a5214b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #parse the input and output texts\n",
    "# for i, (_encode, _decode_input, _decode_target) in enumerate(zip(encode_input, decode_input, decode_target)):\n",
    "#     for t, char in enumerate(_encode):\n",
    "#         encoder_input_data[i, t, char] = 1.\n",
    "#     for t, (char_input, char_target) in enumerate(zip(_decode_input, _decode_target)):\n",
    "#         decoder_input_data[i, t, char_input] = 1.\n",
    "#         decoder_target_data[i, t, char_target] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d39c1a45-55b9-4a1f-b14f-c53cf326d7e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 17:20:48.174020: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#define an input of the encoder with length as the number of encoder tokens\n",
    "encoder_inputs = Input(shape=(None, VOCAB_SIZE))\n",
    "#instantiate the LSTM model\n",
    "encoder = Bidirectional(LSTM(EMBEDDING_DIM, return_state=True))\n",
    "#define the outputs and states of the encoder\n",
    "#encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "#disregard encoder_outputs and keep only the states\n",
    "#encoder_states = [state_h, state_c]\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0931c5c4-2762-4369-86b3-2e5b6d4a81a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, VOCAB_SIZE))    \n",
    "decoder_lstm = LSTM(EMBEDDING_DIM*2, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3349fef-508f-4d51-8b82-15a14a73f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #define an input of the encoder with length as the number of encoder tokens\n",
    "# decoder_inputs = Input(shape=(None, VOCAB_SIZE))\n",
    "# #define the LSTM model for the decoder setting the return sequences and return state to True\n",
    "# decoder_lstm = LSTM(EMBEDDING_DIM, return_sequences=True, return_state=True)\n",
    "# #define only the decoder output for the training model. The states are only needed in the inference model\n",
    "# decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# decoder_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# #define the training model which requires the encoder_input_data and decoder_input_data to return the decoder_target_data\n",
    "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04be9ec0-d1fc-4a42-a866-6740f1874fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "\n",
    "#     print([idx2word[k] for k in encode_input[i]])\n",
    "#     print(train[i].split(' '))\n",
    "#     print([idx2word[k] for k in decode_input[i]])\n",
    "#     print(label[i].split(' '))\n",
    "    \n",
    "#     encode_idx = np.argmax(encoder_input_data[i],axis=1)\n",
    "#     print([idx2word[k] for k in encode_idx])\n",
    "#     decode_idx1 = np.argmax(decoder_input_data[i],axis=1)\n",
    "#     print([idx2word[k] for k in decode_idx1])\n",
    "#     decode_idx2 = np.argmax(decoder_target_data[i],axis=1)\n",
    "#     print([idx2word[k] for k in decode_idx2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e2d231f7-0be3-4ce0-80dc-218785af965a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 35)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1474ba1e-729b-470c-b83d-a57c7fd866bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1, 145)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = np.logical_not(np.equal(np.argmax(decoder_input_data[0],axis=1), 0))\n",
    "print(tmp.shape)\n",
    "np.tile(tmp, [1,1,5]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "63b4428d-2302-4995-a28a-bab2a1225e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(targets, pred):\n",
    "    cross_entropy = CategoricalCrossentropy()\n",
    "    mask = tf.math.logical_not(tf.math.equal(tf.math.argmax(pred,axis=2), 0))\n",
    "    mask = tf.expand_dims(mask, axis=-1)\n",
    "    loss = cross_entropy(targets, pred, sample_weight=mask)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2240972a-0156-4941-b5cb-7f559765aac2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6250/6250 - 660s - loss: 0.0349 - accuracy: 0.9872 - val_loss: 0.0287 - val_accuracy: 0.9892 - 660s/epoch - 106ms/step\n",
      "Epoch 2/10\n",
      "6250/6250 - 679s - loss: 0.0294 - accuracy: 0.9885 - val_loss: 0.0243 - val_accuracy: 0.9912 - 679s/epoch - 109ms/step\n",
      "Epoch 3/10\n",
      "6250/6250 - 693s - loss: 0.0267 - accuracy: 0.9895 - val_loss: 0.0254 - val_accuracy: 0.9896 - 693s/epoch - 111ms/step\n",
      "Epoch 4/10\n",
      "6250/6250 - 741s - loss: 0.0246 - accuracy: 0.9903 - val_loss: 0.0215 - val_accuracy: 0.9922 - 741s/epoch - 119ms/step\n",
      "Epoch 5/10\n",
      "6250/6250 - 602s - loss: 0.0229 - accuracy: 0.9909 - val_loss: 0.0209 - val_accuracy: 0.9918 - 602s/epoch - 96ms/step\n",
      "Epoch 6/10\n",
      "6250/6250 - 606s - loss: 0.0214 - accuracy: 0.9915 - val_loss: 0.0196 - val_accuracy: 0.9924 - 606s/epoch - 97ms/step\n",
      "Epoch 7/10\n",
      "6250/6250 - 596s - loss: 0.0202 - accuracy: 0.9920 - val_loss: 0.0176 - val_accuracy: 0.9936 - 596s/epoch - 95ms/step\n",
      "Epoch 8/10\n",
      "6250/6250 - 595s - loss: 0.0191 - accuracy: 0.9924 - val_loss: 0.0164 - val_accuracy: 0.9939 - 595s/epoch - 95ms/step\n",
      "Epoch 9/10\n",
      "6250/6250 - 593s - loss: 0.0180 - accuracy: 0.9929 - val_loss: 0.0152 - val_accuracy: 0.9945 - 593s/epoch - 95ms/step\n",
      "Epoch 10/10\n",
      "6250/6250 - 584s - loss: 0.0172 - accuracy: 0.9932 - val_loss: 0.0139 - val_accuracy: 0.9951 - 584s/epoch - 93ms/step\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "model.compile(optimizer='rmsprop', loss=loss_func, metrics=['accuracy'])\n",
    "callback = EarlyStopping(monitor='loss', patience=1)\n",
    "\n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "         batch_size=128,\n",
    "         epochs=10,\n",
    "         validation_split=0.2, verbose=2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ccacb1e3-1e16-4ea6-b930-5844fd337fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(EMBEDDING_DIM*2,))\n",
    "decoder_state_input_c = Input(shape=(EMBEDDING_DIM*2,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9eacc081-a3bf-4d91-87b3-4fc1f054c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./weights/encoder_model.json', 'w', encoding='utf8') as f:\n",
    "    f.write(encoder_model.to_json())\n",
    "encoder_model.save_weights('./weights/encoder_model_weights.h5')\n",
    "\n",
    "with open('./weights/decoder_model.json', 'w', encoding='utf8') as f:\n",
    "    f.write(decoder_model.to_json())\n",
    "decoder_model.save_weights('./weights/decoder_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "50cccc4a-c51e-4b62-8ba7-02f3d11b9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('network.txt', 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fd50f86a-f2f0-45d9-a201-668635da49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_filename, model_weights_filename):\n",
    "    with open(model_filename, 'r', encoding='utf8') as f:\n",
    "        model = model_from_json(f.read())\n",
    "    model.load_weights(model_weights_filename)\n",
    "    return model\n",
    "\n",
    "encoder = load_model('./weights/encoder_model.json', './weights/encoder_model_weights.h5')\n",
    "decoder = load_model('./weights/decoder_model.json', './weights/decoder_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a8a9f641-1a5d-4ac1-bdd2-4d2463faeb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, VOCAB_SIZE))\n",
    "    target_seq[0, 0, word2idx['#']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder.predict([target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2word[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '$' or len(decoded_sentence) > MAX_LEN):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, VOCAB_SIZE))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cea9d4ab-8c34-48f1-be85-18d440877a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Input sentence: ( 2 1 - 7 * n ) * ( 6 * n + 4 )\n",
      "Decoded sentence: -42*n**2+98*n+84\n",
      "\n",
      "\n",
      "Input sentence: ( 1 4 - 9 * k ) * ( 4 * k + 8 )\n",
      "Decoded sentence: -36*k**2-16*k+112\n",
      "\n",
      "\n",
      "Input sentence: ( 1 3 - s ) * ( 7 * s + 6 )\n",
      "Decoded sentence: -7*s**2+85*s+78\n",
      "\n",
      "\n",
      "Input sentence: 3 * a * ( 4 * a - 6 )\n",
      "Decoded sentence: 12*a**2-18*a\n",
      "\n",
      "\n",
      "Input sentence: ( 1 6 - 5 * s ) * ( s - 3 2 )\n",
      "Decoded sentence: -5*s**2+176*s-512\n",
      "\n",
      "\n",
      "Input sentence: ( 1 3 - 5 * s ) * ( s + 2 )\n",
      "Decoded sentence: -5*s**2+3*s+26\n",
      "\n",
      "\n",
      "Input sentence: ( - 7 * j - 1 4 ) * ( j + 5 )\n",
      "Decoded sentence: -7*j**2-49*j-70\n",
      "\n",
      "\n",
      "Input sentence: 5 * j * ( j + 1 3 )\n",
      "Decoded sentence: 5*j**2+65*j\n",
      "\n",
      "\n",
      "Input sentence: ( 2 2 - 3 * c ) * ( c + 6 )\n",
      "Decoded sentence: -3*c**2+4*c+132\n",
      "\n",
      "\n",
      "Input sentence: ( 1 4 - 5 * t ) * ( t + 2 5 )\n",
      "Decoded sentence: -5*t**2-111*t+350\n",
      "\n",
      "\n",
      "Input sentence: ( 7 * s - 2 6 ) * ( 8 * s + 2 7 )\n",
      "Decoded sentence: 56*s**2-15*s-702\n",
      "\n",
      "\n",
      "Input sentence: ( - o - 2 5 ) * ( o - 8 )\n",
      "Decoded sentence: -o**2-17*o+200\n",
      "\n",
      "\n",
      "Input sentence: ( - o - 2 6 ) * ( - o - 1 2 )\n",
      "Decoded sentence: o**2+38*o+312\n",
      "\n",
      "\n",
      "Input sentence: ( 2 7 - 2 * s ) * ( s + 1 6 )\n",
      "Decoded sentence: -2*s**2-5*s+439\n",
      "\n",
      "\n",
      "Input sentence: 5 * x * ( - 5 * x - 3 2 )\n",
      "Decoded sentence: -25*x**2-160*x\n",
      "\n",
      "\n",
      "Input sentence: ( - 8 * n - 1 ) * ( n - 2 5 )\n",
      "Decoded sentence: -8*n**2+199*n+25\n",
      "\n",
      "\n",
      "Input sentence: - s * ( 1 7 - 8 * s )\n",
      "Decoded sentence: 8*s**2-17*s\n",
      "\n",
      "\n",
      "Input sentence: j * ( 2 * j + 6 )\n",
      "Decoded sentence: 2*j**2+6*j\n",
      "\n",
      "\n",
      "Input sentence: - 7 * s * ( s - 2 4 )\n",
      "Decoded sentence: -7*s**2+168*s\n",
      "\n",
      "\n",
      "Input sentence: - 3 * n ** 2\n",
      "Decoded sentence: -3*n**2\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100,120):\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)[:-1]\n",
    "    print('\\n')\n",
    "    print(f\"Input sentence: {train[seq_index]}\")\n",
    "    print(f\"Decoded sentence: {decoded_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea98b0-f6e2-4c85-a3af-808475e8c8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
